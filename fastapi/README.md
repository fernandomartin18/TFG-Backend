# FastAPI - Ollama Integration

API REST para generaciÃ³n de cÃ³digo usando modelos de IA locales con Ollama.

## ğŸš€ Inicio RÃ¡pido

### InstalaciÃ³n

```bash
# Dar permisos de ejecuciÃ³n a los scripts
chmod +x setup.sh run.sh

# Ejecutar instalaciÃ³n
./setup.sh
```

### EjecuciÃ³n

```bash
# OpciÃ³n 1: Usar el script
./run.sh

# OpciÃ³n 2: Manual
source .venv/bin/activate
uvicorn app.main:app --reload
```

El servidor estarÃ¡ disponible en: **http://localhost:8001**

## ğŸ“‹ Prerequisitos

1. **Python 3.10+**
2. **Ollama** instalado y ejecutÃ¡ndose
   ```bash
   # Instalar Ollama
   # macOS/Linux: https://ollama.ai
   
   # Descargar un modelo
   ollama pull qwen2-vl
   
   # Verificar que Ollama estÃ© corriendo
   ollama list
   ```

## ğŸ—ï¸ Estructura del Proyecto

```
fastapi/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                 # Punto de entrada
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py           # ConfiguraciÃ³n y variables de entorno
â”‚   â”‚   â””â”€â”€ logger.py           # Sistema de logging
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ generate.py         # Endpoint de generaciÃ³n
â”‚   â”‚   â””â”€â”€ models.py           # Endpoint de listado de modelos
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ ollama_service.py   # Servicio de comunicaciÃ³n con Ollama
â”‚   â””â”€â”€ schemas/
â”‚       â””â”€â”€ generate_request.py # Modelos Pydantic
â”œâ”€â”€ .env                        # Variables de entorno
â”œâ”€â”€ requirements.txt            # Dependencias Python
â”œâ”€â”€ setup.sh                    # Script de instalaciÃ³n
â””â”€â”€ run.sh                      # Script de ejecuciÃ³n
```

## ğŸ”§ ConfiguraciÃ³n

Edita el archivo `.env`:

```env
# URL base de Ollama
OLLAMA_BASE_URL=http://localhost:11434

# ConfiguraciÃ³n del servidor
HOST=0.0.0.0
PORT=8001

# CORS (separados por comas)
ALLOWED_ORIGINS=http://localhost:3000,http://localhost:5173

# Nivel de logging
LOG_LEVEL=INFO
```

## ğŸ“¡ Endpoints

### Health Check
```bash
GET /
```

### Listar Modelos
```bash
GET /models/
```

Respuesta:
```json
{
  "models": [
    {
      "name": "qwen2-vl",
      "modified_at": "2024-01-15T10:30:00Z",
      "size": 4700000000
    }
  ]
}
```

### Generar CÃ³digo
```bash
POST /generate/
Content-Type: multipart/form-data

model: string (requerido)
prompt: string (requerido)
image: file (opcional)
```

Ejemplo con curl:
```bash
# Sin imagen
curl -X POST "http://localhost:8001/generate/" \
  -F "model=qwen2-vl" \
  -F "prompt=Crea una clase Python para gestiÃ³n de usuarios"

# Con imagen
curl -X POST "http://localhost:8001/generate/" \
  -F "model=qwen2-vl" \
  -F "prompt=Analiza este diagrama UML" \
  -F "image=@diagrama.png"
```

Respuesta:
```json
{
  "result": "class UserManager:\n    def __init__(self):\n        ..."
}
```

## ğŸ§ª Testing

```bash
# Activar entorno
source .venv/bin/activate

# Instalar dependencias de testing
pip install pytest pytest-asyncio httpx

# Ejecutar tests (cuando estÃ©n implementados)
pytest
```

## ğŸ“ Mejoras Implementadas

âœ… **Type hints completos** - Mejor autocompletado y detecciÃ³n de errores
âœ… **Manejo de errores robusto** - HTTPException con cÃ³digos apropiados
âœ… **Logging detallado** - Trazabilidad completa de requests
âœ… **ValidaciÃ³n de entrada** - LÃ­mite de tamaÃ±o de imagen (10MB)
âœ… **DocumentaciÃ³n automÃ¡tica** - Swagger UI en `/docs`
âœ… **CORS configurable** - Soporte para mÃºltiples orÃ­genes
âœ… **ConfiguraciÃ³n centralizada** - Variables de entorno bien estructuradas
âœ… **Compatibilidad Python 3.9+** - Usando `Optional[]` en lugar de `|`

## ğŸ› SoluciÃ³n de Problemas

### Error: "No se ha podido resolver la importaciÃ³n"
```bash
# AsegÃºrate de tener el entorno virtual activado
source .venv/bin/activate

# Reinstala las dependencias
pip install -r requirements.txt
```

### Error: "Connection refused" al llamar a Ollama
```bash
# Verifica que Ollama estÃ© corriendo
curl http://localhost:11434/api/tags

# Si no estÃ¡ corriendo, inÃ­cialo
ollama serve
```

### El servidor no inicia
```bash
# Verifica que el puerto 8001 no estÃ© en uso
lsof -ti:8001

# Mata el proceso si es necesario
kill -9 $(lsof -ti:8001)
```

## ğŸ“š Recursos

- [DocumentaciÃ³n de FastAPI](https://fastapi.tiangolo.com/)
- [DocumentaciÃ³n de Ollama](https://ollama.ai/docs)
- [Pydantic Documentation](https://docs.pydantic.dev/)

## ğŸ”œ PrÃ³ximos Pasos

- [ ] Implementar tests unitarios
- [ ] AÃ±adir streaming de respuestas
- [ ] Implementar cachÃ© de respuestas
- [ ] AÃ±adir mÃ©tricas y monitoreo
- [ ] Dockerizar la aplicaciÃ³n

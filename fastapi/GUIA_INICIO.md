# ğŸš€ GuÃ­a RÃ¡pida de Inicio

## Â¿QuÃ© hace el servidor al arrancarse?

### Al iniciar el servidor FastAPI:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Carga la configuraciÃ³n (.env)          â”‚
â”‚  2. Inicializa FastAPI                     â”‚
â”‚  3. Configura CORS                         â”‚
â”‚  4. Registra los endpoints                 â”‚
â”‚  5. Imprime logs de inicio                 â”‚
â”‚  6. Espera peticiones HTTP                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**âš ï¸ IMPORTANTE:** El servidor **NO se conecta a Ollama** al iniciar.
Solo se conecta cuando alguien llama a los endpoints `/models/` o `/generate/`.

---

## ğŸ¯ 3 Opciones para Iniciar

### OpciÃ³n 1: Inicio AutomÃ¡tico (RECOMENDADO) ğŸŒŸ

```bash
./start.sh
```

**Â¿QuÃ© hace?**
- âœ… Verifica que Ollama estÃ© instalado
- âœ… Inicia Ollama si no estÃ¡ corriendo
- âœ… Verifica modelos instalados
- âœ… Te pregunta si quieres descargar un modelo (si no tienes)
- âœ… Arranca el servidor FastAPI

---

### OpciÃ³n 2: Inicio Manual Completo

**Paso 1 - Iniciar Ollama:**
```bash
# OpciÃ³n A: En primer plano (verÃ¡s logs)
ollama serve

# OpciÃ³n B: Como servicio (segundo plano, macOS)
brew services start ollama
```

**Paso 2 - (Opcional) Instalar un modelo:**
```bash
# Para anÃ¡lisis de diagramas + cÃ³digo (multimodal)
ollama pull qwen2-vl

# Solo para cÃ³digo (mÃ¡s rÃ¡pido)
ollama pull codellama
```

**Paso 3 - Arrancar servidor:**
```bash
./run.sh
```

---

### OpciÃ³n 3: Solo Servidor (sin Ollama)

```bash
./run.sh
```

**FuncionarÃ¡:**
- âœ… `GET /` â†’ Health check
- âœ… `GET /docs` â†’ DocumentaciÃ³n Swagger

**NO funcionarÃ¡:**
- âŒ `GET /models/` â†’ Error 503
- âŒ `POST /generate/` â†’ Error 503

---

## ğŸ” Verificar Estado Actual

### Verificar Ollama:
```bash
./check_ollama.sh
```

Muestra:
- âœ…/âŒ Si Ollama estÃ¡ instalado
- âœ…/âŒ Si Ollama estÃ¡ corriendo
- ğŸ“¦ Modelos instalados
- ğŸ’¡ Sugerencias de modelos

### Verificar Servidor:
```bash
# Una vez iniciado, desde otra terminal:
curl http://localhost:8001/
```

---

## ğŸ“Š Flujo de EjecuciÃ³n

```
Usuario hace request â†’ FastAPI â†’ Ollama â†’ Respuesta
                          â†“
                    Â¿Ollama corriendo?
                          â†“
                   SÃ­          No
                    â†“           â†“
              Responde      Error 503
```

---

## ğŸ§ª Probar sin Ollama

Si solo quieres probar que el servidor FastAPI funciona:

```bash
# Terminal 1: Arrancar servidor
./run.sh

# Terminal 2: Probar health check
curl http://localhost:8001/

# Resultado esperado:
# {"message":"FastAPI IA service running","status":"ok"}
```

---

## ğŸ“ Ejemplo Completo de Uso

```bash
# 1. Verificar estado
./check_ollama.sh

# 2. Si Ollama no estÃ¡ corriendo, iniciarlo
ollama serve  # o: brew services start ollama

# 3. (Primera vez) Instalar un modelo
ollama pull qwen2-vl

# 4. Arrancar servidor
./start.sh

# 5. Probar endpoints (en otra terminal)
# Health check
curl http://localhost:8001/

# Listar modelos
curl http://localhost:8001/models/

# Generar cÃ³digo
curl -X POST "http://localhost:8001/generate/" \
  -F "model=qwen2-vl" \
  -F "prompt=Crea una clase Python para gestionar usuarios"
```

---

## ğŸ›‘ Detener Todo

```bash
# Detener servidor FastAPI
Ctrl + C  (en la terminal del servidor)

# Detener Ollama (si se iniciÃ³ como servicio)
brew services stop ollama

# Detener Ollama (si se iniciÃ³ con ollama serve)
Ctrl + C  (en la terminal de Ollama)
```

---

## ğŸ’¡ RecomendaciÃ³n

**Para desarrollo:**
1. Usa `./start.sh` â†’ lo hace todo automÃ¡ticamente
2. Abre http://localhost:8001/docs â†’ DocumentaciÃ³n interactiva
3. Prueba los endpoints desde Swagger UI

**Para producciÃ³n:**
- Inicia Ollama como servicio: `brew services start ollama`
- Usa un gestor de procesos como `systemd` o `supervisor` para FastAPI

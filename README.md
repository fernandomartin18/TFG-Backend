# Backend de la aplicaci√≥n del TFG

Backend de la aplicaci√≥n que genera c√≥digo h√≠brido a partir de **diagramas de clases** o **descripciones textuales** usando modelos de IA locales con Ollama.

## üìë √çndice

- [üöÄ Inicio R√°pido](#-inicio-r√°pido)
- [üìÇ Estructura del Proyecto](#-estructura-del-proyecto)
- [üß† Arquitectura de dos capas](#-arquitectura-de-dos-capas)
- [‚öôÔ∏è Qu√© hace cada m√≥dulo](#Ô∏è-qu√©-hace-cada-m√≥dulo)
- [üìã Requisitos Previos](#-requisitos-previos)
- [üîê Configuraci√≥n de Variables de Entorno](#-configuraci√≥n-de-variables-de-entorno)
- [üöÄ Instalaci√≥n y Ejecuci√≥n Local](#-instalaci√≥n-y-ejecuci√≥n-local)
- [üß™ Endpoints Disponibles](#-endpoints-disponibles)
- [üß∞ Estructura de Respuesta de Ollama](#-estructura-de-respuesta-de-ollama)
- [üîÑ Flujo de Ejecuci√≥n](#-flujo-de-ejecuci√≥n)
- [üõ†Ô∏è Soluci√≥n de Problemas Comunes](#Ô∏è-soluci√≥n-de-problemas-comunes)
- [üìö Recursos](#-recursos)
- [üìä Tecnolog√≠as Utilizadas](#-tecnolog√≠as-utilizadas)
- [üìÑ Licencia](#-licencia)
- [üìß Contacto](#-contacto)

---

## üöÄ Inicio R√°pido

### Arquitectura del Backend

El backend est√° dividido en dos capas:
- **Node.js API Gateway** (Puerto 3000): Punto de entrada para el frontend, maneja autenticaci√≥n y orquestaci√≥n
- **FastAPI** (Puerto 8001): Procesamiento de IA y comunicaci√≥n con Ollama

```bash
# Terminal 1: Iniciar Ollama
ollama serve

# Terminal 2: Iniciar servidor FastAPI
cd backend/llmapi
./setup.sh  # Solo la primera vez
./run.sh

# Terminal 3: Iniciar API Gateway de Node.js
cd backend
npm install  # Solo la primera vez
npm run dev
```

Los servidores estar√°n disponibles en:
- **Node.js API Gateway**: http://localhost:3000
- **FastAPI**: http://localhost:8001
- **API Docs (FastAPI)**: http://localhost:8001/docs
- **ReDoc (FastAPI)**: http://localhost:8001/redoc

---

## üìÇ Estructura del proyecto

```
backend/
‚îú‚îÄ‚îÄ src/                          # API Gateway (Express + Node.js)
‚îÇ   ‚îú‚îÄ‚îÄ server.js                # Punto de entrada del servidor Express
‚îÇ   ‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ index.js             # Configuraci√≥n centralizada
‚îÇ   ‚îú‚îÄ‚îÄ routes/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ index.js             # Registro de rutas
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models.routes.js     # Rutas de modelos
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ generate.routes.js   # Rutas de generaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ controllers/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models.controller.js     # L√≥gica de modelos
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ generate.controller.js   # L√≥gica de generaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ollama.service.js    # Cliente HTTP para FastAPI
‚îÇ   ‚îú‚îÄ‚îÄ middlewares/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ error.middleware.js  # Manejo de errores
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îî‚îÄ‚îÄ logger.js            # Sistema de logging
‚îú‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ .env
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ README.md
‚îÇ
‚îî‚îÄ‚îÄ llmapi/                       # Backend de IA (FastAPI + Python)
    ‚îú‚îÄ‚îÄ app/
    ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îú‚îÄ‚îÄ main.py
    ‚îÇ   ‚îú‚îÄ‚îÄ routes/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ generate.py      # Endpoint de generaci√≥n de c√≥digo
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ models.py        # Endpoint de modelos
    ‚îÇ   ‚îú‚îÄ‚îÄ services/
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ollama_service.py # Comunicaci√≥n con Ollama
    ‚îÇ   ‚îú‚îÄ‚îÄ schemas/
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ generate_request.py # Modelos Pydantic
    ‚îÇ   ‚îî‚îÄ‚îÄ core/
    ‚îÇ       ‚îú‚îÄ‚îÄ config.py        # Variables de entorno
    ‚îÇ       ‚îî‚îÄ‚îÄ logger.py        # Logging
    ‚îú‚îÄ‚îÄ requirements.txt
    ‚îú‚îÄ‚îÄ setup.sh
    ‚îú‚îÄ‚îÄ run.sh
    ‚îú‚îÄ‚îÄ .env
    ‚îî‚îÄ‚îÄ .venv/
```

---

## üß† Arquitectura de dos capas

### üü¢ Node.js API Gateway (Puerto 3000)
Punto de entrada para el frontend. Se encarga de:
- Recibir peticiones del frontend React
- Validaci√≥n inicial y manejo de archivos
- Proxy a FastAPI para procesamiento de IA
- Futuras features: autenticaci√≥n, historial de chats, gesti√≥n de usuarios

### üîµ FastAPI (Puerto 8001)
Backend especializado en IA. Se encarga de:
- Comunicaci√≥n con **Ollama** (modelos locales de IA)
- **Generaci√≥n en dos pasos**: Extracci√≥n de PlantUML ‚Üí Generaci√≥n de c√≥digo
- Procesamiento de im√°genes y conversi√≥n a base64
- **Validaci√≥n de diagramas UML** en im√°genes
- Generaci√≥n de c√≥digo h√≠brido con contexto conversacional
- Gesti√≥n de modelos y timeouts configurables
- **Streaming con eventos de control** para mostrar progreso paso a paso
- Mantenimiento de historial de mensajes para coherencia en la conversaci√≥n

---

## üß† FastAPI

Se encarga de comunicarse con **Ollama** (modelos locales de IA), procesar imaÃÅgenes y devolver respuestas estructuradas que posteriormente usaraÃÅ el **API Gateway en Node.js** y el **frontend en React**.

## ‚öôÔ∏è Qu√© hace cada m√≥dulo

| Archivo | Descripci√≥n |
|----------|--------------|
| `main.py` | Punto de entrada del servidor FastAPI. Registra las rutas, configura CORS y eventos de startup/shutdown. |
| `routes/generate.py` | Endpoint `/generate/` que recibe `model`, `prompt` y una `imagen` opcional. Valida entrada (max 10MB), llama a Ollama y devuelve el c√≥digo generado. |
| `routes/models.py` | Endpoint `/models/` que devuelve la lista de modelos disponibles en Ollama con manejo de errores robusto. |
| `services/ollama_service.py` | L√≥gica de comunicaci√≥n con Ollama (`/api/chat`, `/api/tags`). Convierte im√°genes a base64 y maneja m√∫ltiples formatos de respuesta. |
| `schemas/generate_request.py` | Modelos Pydantic para validaci√≥n de datos: `GenerateRequest`, `GenerateResponse`, `ModelInfo`, `ModelsResponse`. |
| `core/config.py` | Carga variables de entorno, configura URLs de Ollama y CORS con soporte para m√∫ltiples or√≠genes. |
| `core/logger.py` | Sistema de logging configurable con niveles y formato mejorado. |
| `requirements.txt` | Dependencias de Python necesarias. |

---

## üìã Requisitos Previos

1. **Python 3.10+**
2. **Pip** actualizado: `python -m pip install --upgrade pip`
3. **Ollama** instalado y ejecut√°ndose localmente
   - üëâ [Descargar Ollama](https://ollama.ai)
   - Verificar instalaci√≥n: `ollama --version`
4. **Al menos un modelo descargado**

## üîê Configuraci√≥n de Variables de Entorno

### Node.js API Gateway (.env en la ra√≠z)

Crea un archivo `.env` en la carpeta ra√≠z de `backend/`:

```env
# FastAPI Backend
FASTAPI_URL=http://localhost:8001

# Server Configuration
PORT=3000
NODE_ENV=development

# Request Configuration
REQUEST_TIMEOUT=600000  # 10 minutos en ms (para modelos grandes)
MAX_FILE_SIZE=10485760  # 10MB en bytes

# CORS Configuration (comma-separated)
ALLOWED_ORIGINS=http://localhost:5173,http://localhost:5174

# Logging
LOG_LEVEL=info
```

### FastAPI Backend (.env en llmapi/)

Crea un archivo `.env` en la carpeta `llmapi/`:

```env
# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434

# Timeout Configuration (in seconds)
# For large models (27B+), increase OLLAMA_TIMEOUT to 600-900 seconds
OLLAMA_TIMEOUT=600
OLLAMA_TAGS_TIMEOUT=30

# Server Configuration
HOST=0.0.0.0
PORT=8001

# CORS Configuration (comma-separated list)
ALLOWED_ORIGINS=http://localhost:3000,http://localhost:5173

# Logging
LOG_LEVEL=INFO
```

### ‚öôÔ∏è Configuraci√≥n de Timeouts

- **REQUEST_TIMEOUT** (Node.js): Tiempo m√°ximo en milisegundos para requests HTTP
- **OLLAMA_TIMEOUT** (FastAPI): Tiempo m√°ximo de espera para generaci√≥n de c√≥digo
  - Modelos peque√±os (7B): 60-120 segundos
  - Modelos medianos (13B): 120-300 segundos
  - Modelos grandes (27B+): 600-900 segundos
  
- **OLLAMA_TAGS_TIMEOUT** (FastAPI): Tiempo de espera para listar modelos (default: 30s)

**Nota:** Los scripts de setup crean autom√°ticamente estos archivos si no existen.

---

## üöÄ Instalaci√≥n y Ejecuci√≥n Local

### üì¶ Instalaci√≥n completa (ambos servicios)

```bash
# 1. Instalar y configurar FastAPI
cd backend/llmapi
./setup.sh  # Crea .venv, instala dependencias, crea .env
cd ../..

# 2. Instalar y configurar Node.js
cd backend
npm install  # Instala todas las dependencias
```

### ‚ñ∂Ô∏è Ejecuci√≥n completa

```bash
# Terminal 1: Iniciar Ollama (debe estar corriendo siempre)
ollama serve

# Terminal 2: Iniciar FastAPI
cd backend/llmapi
./run.sh

# Terminal 3: Iniciar Node.js API Gateway
cd backend
npm run dev
```

### üîµ FastAPI - Instalaci√≥n Manual

#### 1Ô∏è‚É£ Navegar a la carpeta

```bash
cd backend/llmapi
```

#### 2Ô∏è‚É£ Crear entorno virtual

```bash
python -m venv .venv
```

#### 3Ô∏è‚É£ Activar el entorno

**macOS / Linux:**
```bash
source .venv/bin/activate
```

**Windows (PowerShell):**
```bash
.venv\Scripts\Activate.ps1
```

#### 4Ô∏è‚É£ Instalar dependencias

```bash
pip install -r requirements.txt
```

#### 5Ô∏è‚É£ Crear archivo `.env`

Copia el ejemplo de la secci√≥n anterior.

### Ejecuci√≥n de FastAPI

**Opci√≥n A: Script autom√°tico**
```bash
./run.sh
```

**Opci√≥n B: Manual**
```bash
source .venv/bin/activate
uvicorn app.main:app --host 0.0.0.0 --port 8001 --reload
```

---

## üß™ Endpoints disponibles

### üåê Node.js API Gateway (Puerto 3000)

Todos los endpoints del frontend deben apuntar a `http://localhost:3000/api`

#### üü¢ GET /api/models

**Descripci√≥n:** Obtiene la lista de modelos disponibles en Ollama

**Request:**
```bash
curl http://localhost:3000/api/models
```

**Response:**
```json
{
  "models": [
    {
      "name": "gemma3:27b",
      "modified_at": "2024-01-15T10:30:00Z",
      "size": 27000000000
    }
  ]
}
```

#### üü¢ POST /api/generate

**Descripci√≥n:** Genera c√≥digo a partir de texto o im√°genes. Soporta hasta 5 im√°genes simult√°neas.

**Request con texto:**
```bash
curl -X POST http://localhost:3000/api/generate \
  -F "model=qwen2.5-coder:14b" \
  -F "prompt=Crea un hola mundo en python"
```

**Request con m√∫ltiples im√°genes:**
```bash
curl -X POST http://localhost:3000/api/generate \
  -F "model=qwen3-vl:8b" \
  -F "prompt=Analiza estos diagramas y genera el c√≥digo" \
  -F "images=@./diagram1.png" \
  -F "images=@./diagram2.png" \
  -F "images=@./diagram3.png"
```

**Response:**
```json
{
  "model": "gemma3:27b",
  "content": "public class User { ... }"
}
```

#### üü¢ POST /api/generate/stream

**Descripci√≥n:** Genera c√≥digo con streaming (respuesta progresiva en tiempo real). Soporta contexto de conversaci√≥n para mantener coherencia entre mensajes y hasta 5 im√°genes simult√°neas.

**Modo Autom√°tico con Im√°genes:**
Cuando se usa el modelo autom√°tico (`Auto`) con im√°genes, el sistema ejecuta un proceso de dos pasos:
1. **Extracci√≥n de PlantUML** usando `qwen3-vl:8b` - Convierte diagramas UML en c√≥digo PlantUML
2. **Generaci√≥n de c√≥digo** usando `qwen2.5-coder:14b` - Genera c√≥digo a partir del PlantUML

Si todas las im√°genes se detectan como no-UML, el proceso se detiene y retorna un mensaje de error.

**Par√°metros:**
- `model` (string, requerido): Nombre del modelo (usa "Auto" para activar el modo de dos pasos)
- `prompt` (string, requerido): Texto del prompt
- `messages` (string, opcional): Historial de mensajes en formato JSON para mantener contexto
- `images` (files, opcional): Hasta 5 im√°genes (m√°x 10MB cada una)
- `autoMode` (string, opcional): "true" para activar el proceso de dos pasos

**Request b√°sico:**
```bash
curl -X POST http://localhost:3000/api/generate/stream \
  -F "model=qwen2.5-coder:14b" \
  -F "prompt=Crea un hola mundo en python"
```

**Request con contexto:**
```bash
curl -X POST http://localhost:3000/api/generate/stream \
  -F "model=qwen2.5-coder:14b" \
  -F "prompt=Ahora hazlo en Java" \
  -F 'messages=[{"role":"user","content":"Crea un hola mundo en python"},{"role":"assistant","content":"def hello_world()..."}]'
```

**Response est√°ndar:** Server-Sent Events (SSE)
```
data: def
data:  hello
data: _world
data: ():
data: \n
data:     print
data: ("
data: Hello
data:  World
data: ")
data: [DONE]
```

**Response en modo autom√°tico con im√°genes:** Server-Sent Events con eventos de control
```
data: "[STEP1_START]"
data: "```plantuml"
data: "\n@startuml\n"
data: "class User {\n"
data: "  +name: String\n"
data: "}\n"
data: "@enduml\n"
data: "```"
data: "[STEP1_END]"
data: "[STEP2_START]"
data: "class User:\n"
data: "    def __init__(self, name):\n"
data: "        self.name = name\n"
data: [DONE]
```

**Eventos de control:**
- `[STEP1_START]`: Inicia extracci√≥n de PlantUML (frontend muestra "Generando PlantUML")
- `[STEP1_END]`: Finaliza extracci√≥n de PlantUML
- `[STEP2_START]`: Inicia generaci√≥n de c√≥digo final
- `[DONE]`: Proceso completado

**Nota:** El streaming permite mostrar la respuesta en tiempo real a medida que el modelo la genera, mejorando la experiencia de usuario para respuestas largas. En modo autom√°tico, el usuario puede ver el PlantUML intermedio de forma expandible.

#### üü¢ POST /api/models/unload

**Descripci√≥n:** Descarga un modelo de la memoria

**Request:**
```bash
curl -X POST http://localhost:3000/api/models/unload \
  -H "Content-Type: application/json" \
  -d '{"model": "qwen3-vl:8b"}'
```

### üîß FastAPI (Puerto 8001)

Endpoints directos de FastAPI (normalmente solo usados internamente por el API Gateway)

#### üü¢ GET / - Health Check


**Descripci√≥n:** Verifica que el servidor est√° activo.

**Ejemplo:**
```bash
curl http://localhost:8001/
```

**Respuesta:**
```json
{
  "message": "FastAPI IA service running",
  "status": "ok"
}
```

---

### üü£ GET /models/ - Listar Modelos

**Descripci√≥n:** Devuelve la lista de modelos disponibles en Ollama.

**Ejemplo:**
```bash
curl http://localhost:8001/models/
```

**Respuesta esperada:**
```json
{
  "models": [
    {
      "name": "qwen2-vl:latest",
      "modified_at": "2024-01-15T10:30:00Z",
      "size": 4400000000,
      "digest": "sha256:abc123..."
    }
  ]
}
```

**Errores posibles:**
- `503` - Ollama no est√° corriendo o no responde

---

### üóëÔ∏è POST /models/unload - Descargar Modelo de Memoria

**Descripci√≥n:** Descarga un modelo de la memoria RAM/VRAM para liberar recursos del sistema.

**Request Body:**
```json
{
  "model": "qwen2-vl"
}
```

**Ejemplo:**
```bash
curl -X POST "http://localhost:8001/models/unload" \
  -H "Content-Type: application/json" \
  -d '{"model": "qwen3-vl:8b"}'
```

**Respuesta esperada:**
```json
{
  "success": true,
  "message": "Modelo qwen2-vl descargado de memoria exitosamente",
  "model": "qwen2-vl"
}
```

**Errores posibles:**
- `503` - Ollama no est√° corriendo o el modelo no existe
- `500` - Error al descargar el modelo

---

### üîµ POST /generate/ - Generar C√≥digo

**Descripci√≥n:** Genera c√≥digo a partir de un prompt y opcionalmente hasta 5 im√°genes.

**Par√°metros:**
- `model` (string, requerido): Nombre del modelo en Ollama
- `prompt` (string, requerido): Descripci√≥n de lo que quieres generar
- `images` (files, opcional): Hasta 5 im√°genes del diagrama UML (m√°x 10MB cada una)

**Ejemplo sin imagen:**
```bash
curl -X POST "http://localhost:8001/generate/" \
  -F "model=qwen3-vl:8b" \
  -F "prompt=Crea una hola mundo en python"
```

**Ejemplo con m√∫ltiples im√°genes:**
```bash
curl -X POST "http://localhost:8001/generate/" \
  -F "model=qwen3-vl:8b" \
  -F "prompt=Analiza los diagramas UML y genera el c√≥digo correspondiente" \
  -F "images=@/ruta/a/diagrama1.png" \
  -F "images=@/ruta/a/diagrama2.png"
```

**Respuesta esperada:**
```json
{
  "result": "class UserManager:\n    def __init__(self):\n        self.users = []\n    \n    def create_user(self, name, email):\n        ..."
}
```

**Errores posibles:**
- `400` - Imagen demasiado grande (>10MB), m√°s de 5 im√°genes, o par√°metros inv√°lidos
- `500` - Error en la generaci√≥n o modelo no disponible
- `503` - Ollama no est√° corriendo

---

### üîµ POST /generate/stream - Generar C√≥digo con Streaming

**Descripci√≥n:** Genera c√≥digo con streaming usando Server-Sent Events (SSE), mostrando la respuesta en tiempo real a medida que se genera. Soporta contexto de conversaci√≥n para recordar mensajes anteriores y hasta 5 im√°genes simult√°neas.

**Modo Autom√°tico (auto_mode=true con im√°genes):**
- **Paso 1**: Extrae PlantUML usando `qwen3-vl:8b` y env√≠a eventos `[STEP1_START]`/`[STEP1_END]`
- **Validaci√≥n**: Si todas las im√°genes son "No diagram", retorna error sin continuar al paso 2
- **Paso 2**: Genera c√≥digo con `qwen2.5-coder:14b` usando el PlantUML extra√≠do

**Par√°metros:**
- `model` (string, requerido): Nombre del modelo en Ollama
- `prompt` (string, requerido): Descripci√≥n de lo que quieres generar
- `messages` (string, opcional): Historial de mensajes en formato JSON para contexto conversacional
- `images` (files, opcional): Hasta 5 im√°genes del diagrama UML (m√°x 10MB cada una)
- `auto_mode` (string, opcional): "true" para activar generaci√≥n en dos pasos

**Ejemplo b√°sico:**
```bash
curl -X POST "http://localhost:8001/generate/stream" \
  -F "model=qwen3-vl:8b" \
  -F "prompt=Crea una clase Usuario en Python"
```

**Ejemplo con contexto:**
```bash
curl -X POST "http://localhost:8001/generate/stream" \
  -F "model=qwen2.5-coder:14b" \
  -F "prompt=Ahora a√±ade un m√©todo para validar el email" \
  -F 'messages=[{"role":"user","content":"Crea una clase Usuario"},{"role":"assistant","content":"class User:..."}]'
```

**Respuesta esperada (SSE):**
```
data: class
data:  User
data: Manager
data: :
data: \n
data:     def
data:  __init__
data: ...
data: [DONE]
```

**Errores posibles:**
- `400` - Imagen demasiado grande (>10MB), m√°s de 5 im√°genes, o par√°metros inv√°lidos
- `500` - Error en la generaci√≥n o modelo no disponible
- `503` - Ollama no est√° corriendo

---

### üìö Documentaci√≥n Interactiva

FastAPI genera documentaci√≥n autom√°tica:

- **Swagger UI**: http://localhost:8001/docs
- **ReDoc**: http://localhost:8001/redoc

Desde Swagger UI puedes probar todos los endpoints directamente.

---

## üß∞ Estructura de respuesta de Ollama

Este servicio interpreta autom√°ticamente las respuestas m√°s comunes del API de Ollama:

**Formato 1:**
```json
{
  "message": {
    "content": "c√≥digo generado..."
  }
}
```

**Formato 2:**
```json
{
  "choices": [
    {
      "message": {
        "content": "c√≥digo generado..."
      }
    }
  ]
}
```

**Formato 3:**
```json
{
  "response": "c√≥digo generado..."
}
```

El servicio maneja autom√°ticamente estos formatos y extrae el contenido en la funci√≥n `_extract_content()` de `app/routes/generate.py`.

---

## üîÑ Flujo de Ejecuci√≥n

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Usuario   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ POST /generate/
       ‚îÇ (model, prompt, image?)
       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  FastAPI Server  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Validaci√≥n ‚îÇ  ‚îÇ ‚Üê Tama√±o imagen, par√°metros
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ         ‚Üì        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Ollama    ‚îÇ  ‚îÇ ‚Üê Convierte imagen a base64
‚îÇ  ‚îÇ  Service   ‚îÇ  ‚îÇ   Construye payload
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ HTTP POST
          ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Ollama  ‚îÇ ‚Üê Modelo de IA local
    ‚îÇ  Server  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ Respuesta JSON
           ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Extract    ‚îÇ ‚Üê Maneja 3 formatos
    ‚îÇ  Content    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Response   ‚îÇ ‚Üí {"result": "c√≥digo..."}
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```
---

## ÔøΩ Soluci√≥n de Problemas Comunes

### Timeout con modelos grandes

**Problema:** Error de timeout al usar modelos grandes (gemma3:27b, llama3:70b, etc.)

**S√≠ntomas:**
```
requests.exceptions.ReadTimeout: HTTPConnectionPool(host='localhost', port=11434): 
Read timed out. (read timeout=600)
```

**Soluci√≥n:**

1. **Aumentar el timeout en `.env`:**
```env
# Para modelos 27B+
OLLAMA_TIMEOUT=900

# Para modelos 70B+
OLLAMA_TIMEOUT=1800
```

2. **Reiniciar el servidor:**
```bash
# Detener el servidor (Ctrl+C)
# Reiniciar
./run.sh
```

3. **Verificar que se aplic√≥:**
```bash
# Los logs deber√≠an mostrar:
# "Calling Ollama with model: gemma3:27b (timeout: 900s)"
```

**Recomendaciones por tama√±o de modelo:**
- **7B** (llama3.2, mistral): 60-120 segundos
- **13B** (llama3.1:13b): 120-300 segundos
- **27B** (gemma3:27b): 600-900 segundos
- **70B+** (llama3:70b): 1200-1800 segundos (20-30 min)

---

### Request timeout en el cliente

**Problema:** El cliente (curl, navegador) se desconecta antes de recibir respuesta.

**Soluci√≥n con curl:**
```bash
# Aumentar el timeout del cliente
curl --max-time 900 -X POST "http://localhost:8001/generate/" \
  -F "model=gemma3:27b" \
  -F "prompt=Tu prompt" \
  -F "image=@imagen.png"
```

**Soluci√≥n en c√≥digo JavaScript/TypeScript:**
```javascript
// Aumentar timeout en fetch
const controller = new AbortController();
const timeoutId = setTimeout(() => controller.abort(), 900000); // 15 min

fetch('http://localhost:8001/generate/', {
  method: 'POST',
  body: formData,
  signal: controller.signal
})
  .finally(() => clearTimeout(timeoutId));
```

---

### Imagen demasiado grande

**Error:** `400 - Imagen demasiado grande. M√°ximo 10MB`

**Soluci√≥n:**
1. Reducir el tama√±o de la imagen
2. O modificar el l√≠mite en `app/routes/generate.py`:
```python
# L√≠nea ~35
if len(image_bytes) > 20 * 1024 * 1024:  # Aumentar a 20MB
```

---

### Modelo no cargado/Respuesta lenta

**Problema:** La primera petici√≥n a un modelo tarda mucho.

**Causa:** Ollama carga el modelo en memoria en la primera petici√≥n.

**Soluci√≥n:**
```bash
# Pre-cargar el modelo antes de usarlo
ollama run gemma3:27b "test"
# Ctrl+D para salir

# Ahora el modelo est√° en memoria y responder√° m√°s r√°pido
```

---

### Im√°genes no son diagramas UML

**Error:** `Las im√°genes proporcionadas no se corresponden con diagramas UML`

**Causa:** El modelo `qwen3-vl:8b` detect√≥ que ninguna de las im√°genes proporcionadas contiene diagramas UML v√°lidos.

**Soluci√≥n:**
1. Verificar que las im√°genes sean diagramas UML (clase, secuencia, casos de uso, etc.)
2. Asegurar que los diagramas sean claros y legibles
3. Si es un diagrama UML pero no se detecta, intenta:
   - Mejorar la calidad de la imagen
   - Aumentar el contraste del diagrama
   - Usar formato PNG en lugar de JPG
4. Para texto sin im√°genes, desactiva el modo autom√°tico y usa directamente `qwen2.5-coder:14b`

---

## ÔøΩüìö Recursos

- [Documentaci√≥n de FastAPI](https://fastapi.tiangolo.com/)
- [Documentaci√≥n de Ollama](https://ollama.ai/docs)
- [Pydantic Documentation](https://docs.pydantic.dev/)
- [Modelos disponibles en Ollama](https://ollama.ai/library)
- [Gu√≠a de APIs REST](https://restfulapi.net/)

---

## üìä Tecnolog√≠as Utilizadas

- **FastAPI** 0.100+ - Framework web moderno y r√°pido
- **Pydantic** V2 - Validaci√≥n de datos
- **Uvicorn** - Servidor ASGI
- **Python-multipart** - Manejo de archivos multipart
- **Requests** - Cliente HTTP
- **Python-dotenv** - Gesti√≥n de variables de entorno
- **Pillow** - Procesamiento de im√°genes
- **Ollama** - Servidor de modelos de IA local

---

## üìÑ Licencia

Hacer m√°s adelante

---

## üìß Contacto

- üìß fernandomm1840@gmail.com
# Backend de la aplicaciÃ³n del TFG

Backend de la aplicaciÃ³n que genera cÃ³digo hÃ­brido a partir de **diagramas de clases** o **descripciones textuales** usando modelos de IA locales con Ollama.

## ğŸ“‘ Ãndice

- [ğŸš€ Inicio RÃ¡pido](#-inicio-rÃ¡pido)
- [ğŸ“‹ Requisitos Previos](#-requisitos-previos)
- [ğŸ“‚ Estructura del Proyecto](#-estructura-del-proyecto)
- [âš™ï¸ QuÃ© hace cada mÃ³dulo](#ï¸-quÃ©-hace-cada-mÃ³dulo)
- [ğŸ” Archivo .env](#-archivo-env-no-incluido-en-el-repo)
- [ğŸš€ InstalaciÃ³n y EjecuciÃ³n](#-instalaciÃ³n-y-ejecuciÃ³n-local)
- [ğŸ§ª Endpoints Disponibles](#-endpoints-disponibles)
- [ğŸ§° Estructura de Respuesta de Ollama](#-estructura-de-respuesta-de-ollama)
- [ğŸ”„ Flujo de EjecuciÃ³n](#-flujo-de-ejecuciÃ³n)
- [ğŸ“š Recursos](#-recursos)

---

## ğŸš€ Inicio RÃ¡pido

```bash
# Terminal 1: Iniciar Ollama
ollama serve

# Terminal 2: Iniciar servidor FastAPI
cd fastapi
./setup.sh  # Solo la primera vez
./run.sh
```

El servidor estarÃ¡ disponible en: **http://localhost:8001**
- **API Docs**: http://localhost:8001/docs
- **ReDoc**: http://localhost:8001/redoc

---

## ğŸ“‚ Estructura del proyecto

```
tfg-backend/
â”œâ”€â”€ fastapi/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â”œâ”€â”€ generate.py      # Endpoint principal: genera cÃ³digo a partir de texto o imagen
â”‚   â”‚   â”‚   â””â”€â”€ models.py         # Endpoint auxiliar: lista modelos disponibles en Ollama
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ ollama_service.py # ConexiÃ³n HTTP con Ollama, manejo de imÃ¡genes base64
â”‚   â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â”‚   â””â”€â”€ generate_request.py # Modelos de datos (entrada/salida) con Pydantic
â”‚   â”‚   â””â”€â”€ core/
â”‚   â”‚       â”œâ”€â”€ config.py         # Carga de variables de entorno (.env)
â”‚   â”‚       â””â”€â”€ logger.py         # ConfiguraciÃ³n bÃ¡sica de logging
â”‚   â”‚
â”‚   â”œâ”€â”€ requirements.txt          # Dependencias de Python
â”‚   â”œâ”€â”€ .env                      # Variables de entorno (NO se sube al repositorio)
â”‚   â””â”€â”€ README.md                 # Este archivo
â””â”€â”€ node/                         # API Gateway (Node.js)
```

---

## ğŸ§  FastAPI

Se encarga de comunicarse con **Ollama** (modelos locales de IA), procesar imaÌgenes y devolver respuestas estructuradas que posteriormente usaraÌ el **API Gateway en Node.js** y el **frontend en React**.

## âš™ï¸ QuÃ© hace cada mÃ³dulo

| Archivo | DescripciÃ³n |
|----------|--------------|
| `main.py` | Punto de entrada del servidor FastAPI. Registra las rutas, configura CORS y eventos de startup/shutdown. |
| `routes/generate.py` | Endpoint `/generate/` que recibe `model`, `prompt` y una `imagen` opcional. Valida entrada (max 10MB), llama a Ollama y devuelve el cÃ³digo generado. |
| `routes/models.py` | Endpoint `/models/` que devuelve la lista de modelos disponibles en Ollama con manejo de errores robusto. |
| `services/ollama_service.py` | LÃ³gica de comunicaciÃ³n con Ollama (`/api/chat`, `/api/tags`). Convierte imÃ¡genes a base64 y maneja mÃºltiples formatos de respuesta. |
| `schemas/generate_request.py` | Modelos Pydantic para validaciÃ³n de datos: `GenerateRequest`, `GenerateResponse`, `ModelInfo`, `ModelsResponse`. |
| `core/config.py` | Carga variables de entorno, configura URLs de Ollama y CORS con soporte para mÃºltiples orÃ­genes. |
| `core/logger.py` | Sistema de logging configurable con niveles y formato mejorado. |
| `requirements.txt` | Dependencias de Python necesarias. |

---

## ğŸ“‹ Requisitos Previos

1. **Python 3.10+**
2. **Pip** actualizado: `python -m pip install --upgrade pip`
3. **Ollama** instalado y ejecutÃ¡ndose localmente
   - ğŸ‘‰ [Descargar Ollama](https://ollama.ai)
   - Verificar instalaciÃ³n: `ollama --version`
4. **Al menos un modelo descargado**

## ğŸ” Archivo .env (no incluido en el repo)

Debes crear un archivo `.env` en la raÃ­z de `fastapi/` con este contenido:

```env
# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434

# Server Configuration
HOST=0.0.0.0
PORT=8001

# CORS Configuration (comma-separated list)
ALLOWED_ORIGINS=http://localhost:3000,http://localhost:5173

# Logging
LOG_LEVEL=INFO
```

**Nota:** El script `setup.sh` crea automÃ¡ticamente este archivo si no existe.

---

## ğŸš€ InstalaciÃ³n y ejecuciÃ³n local

### InstalaciÃ³n AutomÃ¡tica (Recomendado)

```bash
cd tfg-backend/fastapi
./setup.sh
```

Esto hace:
1. Verifica Python 3.10+
2. Crea entorno virtual `.venv`
3. Instala todas las dependencias
4. Crea archivo `.env` si no existe

### InstalaciÃ³n Manual

#### 1ï¸âƒ£ Navegar a la carpeta

```bash
cd tfg-backend/fastapi
```

#### 2ï¸âƒ£ Crear entorno virtual

```bash
python -m venv .venv
```

#### 3ï¸âƒ£ Activar el entorno

**macOS / Linux:**
```bash
source .venv/bin/activate
```

**Windows (PowerShell):**
```bash
.venv\Scripts\Activate.ps1
```

#### 4ï¸âƒ£ Instalar dependencias

```bash
pip install -r requirements.txt
```

#### 5ï¸âƒ£ Crear archivo `.env`

Copia el ejemplo de la secciÃ³n anterior.

### EjecuciÃ³n

```bash
./run.sh
```

#### OpciÃ³n C: Manual

```bash
source .venv/bin/activate
uvicorn app.main:app --host 0.0.0.0 --port 8001 --reload
```

El servidor se iniciarÃ¡ en: **http://localhost:8001**

---

## ğŸ§ª Endpoints disponibles

### ğŸŸ¢ GET / - Health Check

**DescripciÃ³n:** Verifica que el servidor estÃ¡ activo.

**Ejemplo:**
```bash
curl http://localhost:8001/
```

**Respuesta:**
```json
{
  "message": "FastAPI IA service running",
  "status": "ok"
}
```

---

### ğŸŸ£ GET /models/ - Listar Modelos

**DescripciÃ³n:** Devuelve la lista de modelos disponibles en Ollama.

**Ejemplo:**
```bash
curl http://localhost:8001/models/
```

**Respuesta esperada:**
```json
{
  "models": [
    {
      "name": "qwen2-vl:latest",
      "modified_at": "2024-01-15T10:30:00Z",
      "size": 4400000000,
      "digest": "sha256:abc123..."
    }
  ]
}
```

**Errores posibles:**
- `503` - Ollama no estÃ¡ corriendo o no responde

---

### ğŸ”µ POST /generate/ - Generar CÃ³digo

**DescripciÃ³n:** Genera cÃ³digo a partir de un prompt y opcionalmente una imagen (diagrama UML).

**ParÃ¡metros:**
- `model` (string, requerido): Nombre del modelo en Ollama
- `prompt` (string, requerido): DescripciÃ³n de lo que quieres generar
- `image` (file, opcional): Imagen del diagrama UML (mÃ¡x 10MB)

**Ejemplo sin imagen:**
```bash
curl -X POST "http://localhost:8001/generate/" \
  -F "model=qwen2-vl" \
  -F "prompt=Crea una clase Python para gestionar usuarios con mÃ©todos CRUD"
```

**Ejemplo con imagen:**
```bash
curl -X POST "http://localhost:8001/generate/" \
  -F "model=qwen2-vl" \
  -F "prompt=Analiza el diagrama UML y genera el cÃ³digo Python correspondiente" \
  -F "image=@/ruta/a/diagrama.png"
```

**Respuesta esperada:**
```json
{
  "result": "class UserManager:\n    def __init__(self):\n        self.users = []\n    \n    def create_user(self, name, email):\n        ..."
}
```

**Errores posibles:**
- `400` - Imagen demasiado grande (>10MB) o parÃ¡metros invÃ¡lidos
- `500` - Error en la generaciÃ³n o modelo no disponible
- `503` - Ollama no estÃ¡ corriendo

---

### ğŸ“š DocumentaciÃ³n Interactiva

FastAPI genera documentaciÃ³n automÃ¡tica:

- **Swagger UI**: http://localhost:8001/docs
- **ReDoc**: http://localhost:8001/redoc

Desde Swagger UI puedes probar todos los endpoints directamente.

---

## ğŸ§° Estructura de respuesta de Ollama

Este servicio interpreta automÃ¡ticamente las respuestas mÃ¡s comunes del API de Ollama:

**Formato 1:**
```json
{
  "message": {
    "content": "cÃ³digo generado..."
  }
}
```

**Formato 2:**
```json
{
  "choices": [
    {
      "message": {
        "content": "cÃ³digo generado..."
      }
    }
  ]
}
```

**Formato 3:**
```json
{
  "response": "cÃ³digo generado..."
}
```

El servicio maneja automÃ¡ticamente estos formatos y extrae el contenido en la funciÃ³n `_extract_content()` de `app/routes/generate.py`.

---

## ğŸ”„ Flujo de EjecuciÃ³n

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Usuario   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ POST /generate/
       â”‚ (model, prompt, image?)
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI Server  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ ValidaciÃ³n â”‚  â”‚ â† TamaÃ±o imagen, parÃ¡metros
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â†“        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Ollama    â”‚  â”‚ â† Convierte imagen a base64
â”‚  â”‚  Service   â”‚  â”‚   Construye payload
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚ HTTP POST
          â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Ollama  â”‚ â† Modelo de IA local
    â”‚  Server  â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
           â”‚ Respuesta JSON
           â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Extract    â”‚ â† Maneja 3 formatos
    â”‚  Content    â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Response   â”‚ â†’ {"result": "cÃ³digo..."}
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
---

## ğŸ“š Recursos

- [DocumentaciÃ³n de FastAPI](https://fastapi.tiangolo.com/)
- [DocumentaciÃ³n de Ollama](https://ollama.ai/docs)
- [Pydantic Documentation](https://docs.pydantic.dev/)
- [Modelos disponibles en Ollama](https://ollama.ai/library)
- [GuÃ­a de APIs REST](https://restfulapi.net/)

---

## ğŸ“Š TecnologÃ­as Utilizadas

- **FastAPI** 0.100+ - Framework web moderno y rÃ¡pido
- **Pydantic** V2 - ValidaciÃ³n de datos
- **Uvicorn** - Servidor ASGI
- **Python-multipart** - Manejo de archivos multipart
- **Requests** - Cliente HTTP
- **Python-dotenv** - GestiÃ³n de variables de entorno
- **Pillow** - Procesamiento de imÃ¡genes
- **Ollama** - Servidor de modelos de IA local

---

## ğŸ“„ Licencia

Hacer mÃ¡s adelante

---

## ğŸ“§ Contacto

- ğŸ“§ fernandomm1840@gmail.com
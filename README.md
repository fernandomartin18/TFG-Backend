# Backend de la aplicaciÃ³n del TFG

Backend de la aplicaciÃ³n que genera cÃ³digo hÃ­brido a partir de **diagramas de clases** o **descripciones textuales** usando modelos de IA locales con Ollama.

## ğŸ“‘ Ãndice

- [ğŸš€ Inicio RÃ¡pido](#-inicio-rÃ¡pido)
- [ğŸ“‹ Requisitos Previos](#-requisitos-previos)
- [ğŸ“‚ Estructura del Proyecto](#-estructura-del-proyecto)
- [âš™ï¸ QuÃ© hace cada mÃ³dulo](#ï¸-quÃ©-hace-cada-mÃ³dulo)
- [ğŸ” Archivo .env](#-archivo-env-no-incluido-en-el-repo)
- [ğŸš€ InstalaciÃ³n y EjecuciÃ³n](#-instalaciÃ³n-y-ejecuciÃ³n-local)
- [ğŸ” VerificaciÃ³n de Estado](#-verificaciÃ³n-de-estado)
- [ğŸ§ª Endpoints Disponibles](#-endpoints-disponibles)
- [ğŸ§° Estructura de Respuesta de Ollama](#-estructura-de-respuesta-de-ollama)
- [ğŸ“ Mejoras Implementadas](#-mejoras-implementadas)
- [ğŸ› SoluciÃ³n de Problemas](#-soluciÃ³n-de-problemas)
- [ğŸ”„ Flujo de EjecuciÃ³n](#-flujo-de-ejecuciÃ³n)
- [âš ï¸ Notas Importantes](#ï¸-notas-importantes)
- [ğŸ§© PrÃ³ximos Pasos](#-prÃ³ximos-pasos)
- [ğŸ§¾ Comandos Ãštiles](#-comandos-Ãºtiles)
- [ğŸ“š Recursos](#-recursos)

---

## ğŸš€ Inicio RÃ¡pido

### OpciÃ³n 1: Inicio AutomÃ¡tico (RECOMENDADO) ğŸŒŸ

```bash
cd fastapi
./start.sh
```

Este script:
- âœ… Verifica e inicia Ollama automÃ¡ticamente
- âœ… Verifica modelos instalados
- âœ… Te pregunta si quieres descargar un modelo (si no tienes)
- âœ… Arranca el servidor FastAPI

### OpciÃ³n 2: Inicio Manual

```bash
# Terminal 1: Iniciar Ollama
ollama serve

# Terminal 2: Iniciar servidor FastAPI
cd fastapi
./setup.sh  # Solo la primera vez
./run.sh
```

El servidor estarÃ¡ disponible en: **http://localhost:8001**
- **API Docs**: http://localhost:8001/docs
- **ReDoc**: http://localhost:8001/redoc

---

## ğŸ“‚ Estructura del proyecto

```
tfg-backend/
â”œâ”€â”€ fastapi/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â”œâ”€â”€ generate.py      # Endpoint principal: genera cÃ³digo a partir de texto o imagen
â”‚   â”‚   â”‚   â””â”€â”€ models.py         # Endpoint auxiliar: lista modelos disponibles en Ollama
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ ollama_service.py # ConexiÃ³n HTTP con Ollama, manejo de imÃ¡genes base64
â”‚   â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â”‚   â””â”€â”€ generate_request.py # Modelos de datos (entrada/salida) con Pydantic
â”‚   â”‚   â””â”€â”€ core/
â”‚   â”‚       â”œâ”€â”€ config.py         # Carga de variables de entorno (.env)
â”‚   â”‚       â””â”€â”€ logger.py         # ConfiguraciÃ³n bÃ¡sica de logging
â”‚   â”‚
â”‚   â”œâ”€â”€ requirements.txt          # Dependencias de Python
â”‚   â”œâ”€â”€ .env                      # Variables de entorno (NO se sube al repositorio)
â”‚   â””â”€â”€ README.md                 # Este archivo
â””â”€â”€ node/                         # API Gateway (Node.js)
```

---

## ğŸ§  FastAPI

Se encarga de comunicarse con **Ollama** (modelos locales de IA), procesar imaÌgenes y devolver respuestas estructuradas que posteriormente usaraÌ el **API Gateway en Node.js** y el **frontend en React**.

## âš™ï¸ QuÃ© hace cada mÃ³dulo

| Archivo | DescripciÃ³n |
|----------|--------------|
| `main.py` | Punto de entrada del servidor FastAPI. Registra las rutas, configura CORS y eventos de startup/shutdown. |
| `routes/generate.py` | Endpoint `/generate/` que recibe `model`, `prompt` y una `imagen` opcional. Valida entrada (max 10MB), llama a Ollama y devuelve el cÃ³digo generado. |
| `routes/models.py` | Endpoint `/models/` que devuelve la lista de modelos disponibles en Ollama con manejo de errores robusto. |
| `services/ollama_service.py` | LÃ³gica de comunicaciÃ³n con Ollama (`/api/chat`, `/api/tags`). Convierte imÃ¡genes a base64 y maneja mÃºltiples formatos de respuesta. |
| `schemas/generate_request.py` | Modelos Pydantic para validaciÃ³n de datos: `GenerateRequest`, `GenerateResponse`, `ModelInfo`, `ModelsResponse`. |
| `core/config.py` | Carga variables de entorno, configura URLs de Ollama y CORS con soporte para mÃºltiples orÃ­genes. |
| `core/logger.py` | Sistema de logging configurable con niveles y formato mejorado. |
| `requirements.txt` | Dependencias de Python necesarias. |

### Scripts de utilidad:

| Script | DescripciÃ³n |
|--------|-------------|
| `setup.sh` | InstalaciÃ³n automÃ¡tica: crea entorno virtual, instala dependencias y configura `.env`. |
| `run.sh` | Ejecuta el servidor FastAPI (requiere `setup.sh` previo). |
| `start.sh` | **Script todo-en-uno**: verifica Ollama, ofrece descargar modelos y arranca el servidor. |
| `check_ollama.sh` | Verifica estado de Ollama, modelos instalados y sugiere modelos recomendados. |
| `test_setup.py` | Verifica que todas las importaciones y configuraciÃ³n funcionan correctamente. |

---

## ğŸ“‹ Requisitos Previos

1. **Python 3.10+**
2. **Pip** actualizado: `python -m pip install --upgrade pip`
3. **Ollama** instalado y ejecutÃ¡ndose localmente
   - ğŸ‘‰ [Descargar Ollama](https://ollama.ai)
   - Verificar instalaciÃ³n: `ollama --version`
4. **Al menos un modelo descargado** (recomendado: `qwen2-vl` para multimodal)

### InstalaciÃ³n de Ollama y modelos:

```bash
# Descargar e instalar Ollama desde https://ollama.ai

# Iniciar Ollama
ollama serve

# En otra terminal, descargar un modelo
# Para anÃ¡lisis de diagramas + cÃ³digo (multimodal, 4.4GB):
ollama pull qwen2-vl

# O solo para cÃ³digo (mÃ¡s rÃ¡pido, 3.8GB):
ollama pull codellama

# Verificar modelos instalados
ollama list
```

---

## ğŸ” Archivo .env (no incluido en el repo)

Debes crear un archivo `.env` en la raÃ­z de `fastapi/` con este contenido:

```env
# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434

# Server Configuration
HOST=0.0.0.0
PORT=8001

# CORS Configuration (comma-separated list)
ALLOWED_ORIGINS=http://localhost:3000,http://localhost:5173

# Logging
LOG_LEVEL=INFO
```

**Nota:** El script `setup.sh` crea automÃ¡ticamente este archivo si no existe.

---

## ğŸš€ InstalaciÃ³n y ejecuciÃ³n local

### InstalaciÃ³n AutomÃ¡tica (Recomendado)

```bash
cd tfg-backend/fastapi
./setup.sh
```

Esto hace:
1. Verifica Python 3.10+
2. Crea entorno virtual `.venv`
3. Instala todas las dependencias
4. Crea archivo `.env` si no existe

### InstalaciÃ³n Manual

#### 1ï¸âƒ£ Navegar a la carpeta

```bash
cd tfg-backend/fastapi
```

#### 2ï¸âƒ£ Crear entorno virtual

```bash
python -m venv .venv
```

#### 3ï¸âƒ£ Activar el entorno

**macOS / Linux:**
```bash
source .venv/bin/activate
```

**Windows (PowerShell):**
```bash
.venv\Scripts\Activate.ps1
```

#### 4ï¸âƒ£ Instalar dependencias

```bash
pip install -r requirements.txt
```

#### 5ï¸âƒ£ Crear archivo `.env`

Copia el ejemplo de la secciÃ³n anterior.

### EjecuciÃ³n

#### OpciÃ³n A: Script automÃ¡tico (verifica Ollama)

```bash
./start.sh
```

#### OpciÃ³n B: Solo servidor

```bash
./run.sh
```

#### OpciÃ³n C: Manual

```bash
source .venv/bin/activate
uvicorn app.main:app --host 0.0.0.0 --port 8001 --reload
```

El servidor se iniciarÃ¡ en: **http://localhost:8001**

---

## ï¿½ VerificaciÃ³n de Estado

### Verificar configuraciÃ³n de Ollama

```bash
./check_ollama.sh
```

Muestra:
- âœ…/âŒ Si Ollama estÃ¡ instalado
- âœ…/âŒ Si Ollama estÃ¡ corriendo
- ğŸ“¦ Modelos instalados
- ğŸ’¡ Sugerencias de modelos recomendados

### Verificar configuraciÃ³n de Python

```bash
source .venv/bin/activate
python test_setup.py
```

Verifica:
- Todas las importaciones
- Schemas Pydantic
- ConfiguraciÃ³n de variables de entorno

### Verificar servidor

```bash
# Health check
curl http://localhost:8001/

# DeberÃ­a devolver:
# {"message":"FastAPI IA service running","status":"ok"}
```

---

## ğŸ§ª Endpoints disponibles

### ğŸŸ¢ GET / - Health Check

**DescripciÃ³n:** Verifica que el servidor estÃ¡ activo.

**Ejemplo:**
```bash
curl http://localhost:8001/
```

**Respuesta:**
```json
{
  "message": "FastAPI IA service running",
  "status": "ok"
}
```

---

### ğŸŸ£ GET /models/ - Listar Modelos

**DescripciÃ³n:** Devuelve la lista de modelos disponibles en Ollama.

**Ejemplo:**
```bash
curl http://localhost:8001/models/
```

**Respuesta esperada:**
```json
{
  "models": [
    {
      "name": "qwen2-vl:latest",
      "modified_at": "2024-01-15T10:30:00Z",
      "size": 4400000000,
      "digest": "sha256:abc123..."
    }
  ]
}
```

**Errores posibles:**
- `503` - Ollama no estÃ¡ corriendo o no responde

---

### ğŸ”µ POST /generate/ - Generar CÃ³digo

**DescripciÃ³n:** Genera cÃ³digo a partir de un prompt y opcionalmente una imagen (diagrama UML).

**ParÃ¡metros:**
- `model` (string, requerido): Nombre del modelo en Ollama
- `prompt` (string, requerido): DescripciÃ³n de lo que quieres generar
- `image` (file, opcional): Imagen del diagrama UML (mÃ¡x 10MB)

**Ejemplo sin imagen:**
```bash
curl -X POST "http://localhost:8001/generate/" \
  -F "model=qwen2-vl" \
  -F "prompt=Crea una clase Python para gestionar usuarios con mÃ©todos CRUD"
```

**Ejemplo con imagen:**
```bash
curl -X POST "http://localhost:8001/generate/" \
  -F "model=qwen2-vl" \
  -F "prompt=Analiza el diagrama UML y genera el cÃ³digo Python correspondiente" \
  -F "image=@/ruta/a/diagrama.png"
```

**Respuesta esperada:**
```json
{
  "result": "class UserManager:\n    def __init__(self):\n        self.users = []\n    \n    def create_user(self, name, email):\n        ..."
}
```

**Errores posibles:**
- `400` - Imagen demasiado grande (>10MB) o parÃ¡metros invÃ¡lidos
- `500` - Error en la generaciÃ³n o modelo no disponible
- `503` - Ollama no estÃ¡ corriendo

---

### ğŸ“š DocumentaciÃ³n Interactiva

FastAPI genera documentaciÃ³n automÃ¡tica:

- **Swagger UI**: http://localhost:8001/docs
- **ReDoc**: http://localhost:8001/redoc

Desde Swagger UI puedes probar todos los endpoints directamente.

---

## ğŸ§° Estructura de respuesta de Ollama

Este servicio interpreta automÃ¡ticamente las respuestas mÃ¡s comunes del API de Ollama:

**Formato 1:**
```json
{
  "message": {
    "content": "cÃ³digo generado..."
  }
}
```

**Formato 2:**
```json
{
  "choices": [
    {
      "message": {
        "content": "cÃ³digo generado..."
      }
    }
  ]
}
```

**Formato 3:**
```json
{
  "response": "cÃ³digo generado..."
}
```

El servicio maneja automÃ¡ticamente estos formatos y extrae el contenido en la funciÃ³n `_extract_content()` de `app/routes/generate.py`.

---

## ğŸ“ Mejoras Implementadas

### Arquitectura y CÃ³digo
- âœ… **Type hints completos** - Mejor autocompletado y detecciÃ³n de errores
- âœ… **Compatibilidad Python 3.9+** - Usando `Optional[]` en lugar de `|`
- âœ… **Docstrings completos** - En todas las funciones con descripciÃ³n de parÃ¡metros
- âœ… **ValidaciÃ³n robusta** - LÃ­mite de tamaÃ±o de imagen (10MB), validaciÃ³n de entrada
- âœ… **Manejo de errores especÃ­fico** - HTTPException con cÃ³digos apropiados (400, 500, 503)

### Logging y Monitoreo
- âœ… **Logging detallado** - Trazabilidad completa de requests
- âœ… **Nivel configurable** - LOG_LEVEL desde variable de entorno
- âœ… **Formato mejorado** - Timestamp, nivel y nombre del mÃ³dulo

### ConfiguraciÃ³n
- âœ… **CORS configurable** - Soporte para mÃºltiples orÃ­genes
- âœ… **Variables centralizadas** - Todas en `.env`
- âœ… **URLs construidas automÃ¡ticamente** - Desde `OLLAMA_BASE_URL`

### DocumentaciÃ³n
- âœ… **OpenAPI/Swagger** - DocumentaciÃ³n automÃ¡tica en `/docs`
- âœ… **Schemas Pydantic** - Con ejemplos para la documentaciÃ³n
- âœ… **Scripts automatizados** - Para instalaciÃ³n, ejecuciÃ³n y verificaciÃ³n

---

## ğŸ› SoluciÃ³n de Problemas

### Error: "No se ha podido resolver la importaciÃ³n"

**Causa:** Dependencias no instaladas o entorno virtual no activado.

**SoluciÃ³n:**
```bash
source .venv/bin/activate
pip install -r requirements.txt
```

O ejecuta:
```bash
./setup.sh
```

---

### Error: "Connection refused" al llamar a Ollama

**Causa:** Ollama no estÃ¡ corriendo.

**SoluciÃ³n:**
```bash
# Verificar que Ollama estÃ© corriendo
curl http://localhost:11434/api/tags

# Si no responde, iniciarlo
ollama serve

# O como servicio (macOS)
brew services start ollama
```

---

### Error: "No hay modelos disponibles"

**Causa:** No tienes modelos descargados en Ollama.

**SoluciÃ³n:**
```bash
# Listar modelos
ollama list

# Si no hay ninguno, descargar uno
ollama pull qwen2-vl  # Multimodal (recomendado)
# O
ollama pull codellama  # Solo cÃ³digo
```

---

### El servidor no inicia (puerto ocupado)

**Causa:** El puerto 8001 estÃ¡ siendo usado por otro proceso.

**SoluciÃ³n:**
```bash
# Ver quÃ© proceso usa el puerto
lsof -ti:8001

# Matar el proceso
kill -9 $(lsof -ti:8001)

# O cambiar el puerto en .env
PORT=8002
```

---

### Imagen demasiado grande

**Error:** `400 - Imagen demasiado grande. MÃ¡ximo 10MB`

**SoluciÃ³n:**
- Reduce el tamaÃ±o de la imagen
- O modifica el lÃ­mite en `app/routes/generate.py` (lÃ­nea ~35)

---

## ğŸ”„ Flujo de EjecuciÃ³n

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Usuario   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ POST /generate/
       â”‚ (model, prompt, image?)
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI Server  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ ValidaciÃ³n â”‚  â”‚ â† TamaÃ±o imagen, parÃ¡metros
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â†“        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Ollama    â”‚  â”‚ â† Convierte imagen a base64
â”‚  â”‚  Service   â”‚  â”‚   Construye payload
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚ HTTP POST
          â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Ollama  â”‚ â† Modelo de IA local
    â”‚  Server  â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
           â”‚ Respuesta JSON
           â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Extract    â”‚ â† Maneja 3 formatos
    â”‚  Content    â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Response   â”‚ â†’ {"result": "cÃ³digo..."}
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš ï¸ Notas Importantes

### Al arrancar el servidor

El servidor FastAPI **NO se conecta a Ollama** al iniciar. Solo:
1. Carga la configuraciÃ³n (`.env`)
2. Inicializa FastAPI y registra endpoints
3. Configura CORS
4. Imprime logs de inicio
5. Espera peticiones HTTP

La conexiÃ³n a Ollama **solo ocurre** cuando se llama a `/models/` o `/generate/`.

### Entorno de desarrollo

- Los errores de Pylance sobre importaciones son normales hasta instalar dependencias
- Usa el entorno virtual `.venv` para evitar conflictos
- El modo `--reload` recarga automÃ¡ticamente al cambiar cÃ³digo

### ProducciÃ³n

- Cambia `HOST` a la IP especÃ­fica o mantÃ©n `0.0.0.0`
- Ajusta `ALLOWED_ORIGINS` con los dominios reales
- Usa un gestor de procesos (systemd, supervisor)
- Considera usar un proxy inverso (nginx)
- Cambia `LOG_LEVEL` a `WARNING` o `ERROR`

---

## ğŸ§© PrÃ³ximos pasos

- [ ] Implementar tests unitarios (pytest)
- [ ] AÃ±adir streaming de respuestas desde Ollama
- [ ] Implementar cachÃ© de respuestas
- [ ] Integrar con el Gateway Node.js
- [ ] AÃ±adir parseo estructurado del cÃ³digo generado
- [ ] AÃ±adir mÃ©tricas y monitoreo
- [ ] Dockerizar la aplicaciÃ³n
- [ ] Implementar rate limiting
- [ ] AÃ±adir autenticaciÃ³n/autorizaciÃ³n

---

## ğŸ§¾ Comandos Ãštiles

| AcciÃ³n | Comando |
|--------|---------|
| **InstalaciÃ³n** | |
| Crear entorno virtual | `python -m venv .venv` |
| Activar entorno (macOS/Linux) | `source .venv/bin/activate` |
| Activar entorno (Windows) | `.venv\Scripts\Activate.ps1` |
| Instalar dependencias | `pip install -r requirements.txt` |
| InstalaciÃ³n completa | `./setup.sh` |
| **Ollama** | |
| Iniciar Ollama | `ollama serve` |
| Iniciar como servicio (macOS) | `brew services start ollama` |
| Listar modelos | `ollama list` |
| Descargar modelo | `ollama pull qwen2-vl` |
| Probar modelo | `ollama run qwen2-vl` |
| **Servidor** | |
| Ejecutar servidor (completo) | `./start.sh` |
| Ejecutar servidor (solo API) | `./run.sh` |
| Ejecutar manualmente | `uvicorn app.main:app --reload` |
| Ver logs en tiempo real | `tail -f uvicorn.log` |
| **VerificaciÃ³n** | |
| Verificar Ollama | `./check_ollama.sh` |
| Verificar configuraciÃ³n | `python test_setup.py` |
| Health check | `curl http://localhost:8001/` |
| Listar modelos | `curl http://localhost:8001/models/` |
| **Testing** | |
| Probar endpoints | Abrir http://localhost:8001/docs |
| Generar cÃ³digo | Ver ejemplos en secciÃ³n de endpoints |
| **Detener** | |
| Detener servidor | `Ctrl + C` |
| Detener Ollama (servicio) | `brew services stop ollama` |
| Detener Ollama (manual) | `Ctrl + C` |

---

## ğŸ“š Recursos

- [DocumentaciÃ³n de FastAPI](https://fastapi.tiangolo.com/)
- [DocumentaciÃ³n de Ollama](https://ollama.ai/docs)
- [Pydantic Documentation](https://docs.pydantic.dev/)
- [Modelos disponibles en Ollama](https://ollama.ai/library)
- [GuÃ­a de APIs REST](https://restfulapi.net/)

---

## ğŸ“Š TecnologÃ­as Utilizadas

- **FastAPI** 0.100+ - Framework web moderno y rÃ¡pido
- **Pydantic** V2 - ValidaciÃ³n de datos
- **Uvicorn** - Servidor ASGI
- **Python-multipart** - Manejo de archivos multipart
- **Requests** - Cliente HTTP
- **Python-dotenv** - GestiÃ³n de variables de entorno
- **Pillow** - Procesamiento de imÃ¡genes
- **Ollama** - Servidor de modelos de IA local

---

## ğŸ‘¥ Desarrollo

### Estructura del proyecto

El proyecto sigue una arquitectura en capas:

- **Routes** (`app/routes/`): Endpoints de la API
- **Services** (`app/services/`): LÃ³gica de negocio
- **Schemas** (`app/schemas/`): Modelos de datos
- **Core** (`app/core/`): ConfiguraciÃ³n y utilidades

### Convenciones de cÃ³digo

- Type hints en todas las funciones
- Docstrings en formato Google
- Logging en puntos estratÃ©gicos
- Manejo explÃ­cito de errores
- ValidaciÃ³n de entrada con Pydantic

### Contribuir

1. Fork el repositorio
2. Crea una rama: `git checkout -b feature/nueva-funcionalidad`
3. Haz commit: `git commit -am 'AÃ±ade nueva funcionalidad'`
4. Push: `git push origin feature/nueva-funcionalidad`
5. Abre un Pull Request

---

## ğŸ“„ Licencia

[Especificar licencia del proyecto]

---

## ğŸ“§ Contacto

[InformaciÃ³n de contacto o links relevantes]
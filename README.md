# Backend de la aplicaciÃ³n del TFG

Backend de la aplicaciÃ³n que genera cÃ³digo hÃ­brido a partir de **diagramas de clases** o **descripciones textuales** usando modelos de IA locales con Ollama.

## ğŸ“‘ Ãndice

- [ğŸš€ Inicio RÃ¡pido](#-inicio-rÃ¡pido)
- [ğŸ“‚ Estructura del Proyecto](#-estructura-del-proyecto)
- [ğŸ§  Arquitectura de dos capas](#-arquitectura-de-dos-capas)
- [âš™ï¸ QuÃ© hace cada mÃ³dulo](#ï¸-quÃ©-hace-cada-mÃ³dulo)
- [ğŸ“‹ Requisitos Previos](#-requisitos-previos)
- [ğŸ” ConfiguraciÃ³n de Variables de Entorno](#-configuraciÃ³n-de-variables-de-entorno)
- [ğŸš€ InstalaciÃ³n y EjecuciÃ³n Local](#-instalaciÃ³n-y-ejecuciÃ³n-local)
- [ğŸ§ª Endpoints Disponibles](#-endpoints-disponibles)
- [ğŸ§° Estructura de Respuesta de Ollama](#-estructura-de-respuesta-de-ollama)
- [ğŸ”„ Flujo de EjecuciÃ³n](#-flujo-de-ejecuciÃ³n)
- [ğŸ› ï¸ SoluciÃ³n de Problemas Comunes](#ï¸-soluciÃ³n-de-problemas-comunes)
- [ğŸ“š Recursos](#-recursos)
- [ğŸ“Š TecnologÃ­as Utilizadas](#-tecnologÃ­as-utilizadas)
- [ğŸ“„ Licencia](#-licencia)
- [ğŸ“§ Contacto](#-contacto)

---

## ğŸš€ Inicio RÃ¡pido

### Arquitectura del Backend

El backend estÃ¡ dividido en dos capas:
- **Node.js API Gateway** (Puerto 3000): Punto de entrada para el frontend, maneja autenticaciÃ³n y orquestaciÃ³n
- **FastAPI** (Puerto 8001): Procesamiento de IA y comunicaciÃ³n con Ollama

```bash
# Terminal 1: Iniciar Ollama
ollama serve

# Terminal 2: Iniciar servidor FastAPI
cd fastapi
./setup.sh  # Solo la primera vez
./run.sh

# Terminal 3: Iniciar API Gateway de Node.js
cd node
npm install  # Solo la primera vez
npm run dev
```

Los servidores estarÃ¡n disponibles en:
- **Node.js API Gateway**: http://localhost:3000
- **FastAPI**: http://localhost:8001
- **API Docs (FastAPI)**: http://localhost:8001/docs
- **ReDoc (FastAPI)**: http://localhost:8001/redoc

---

## ğŸ“‚ Estructura del proyecto

```
tfg-backend/
â”œâ”€â”€ node/                         # API Gateway (Express + Node.js)
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ server.js            # Punto de entrada del servidor Express
â”‚   â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”‚   â””â”€â”€ index.js         # ConfiguraciÃ³n centralizada
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â”œâ”€â”€ index.js         # Registro de rutas
â”‚   â”‚   â”‚   â”œâ”€â”€ models.routes.js # Rutas de modelos
â”‚   â”‚   â”‚   â””â”€â”€ generate.routes.js # Rutas de generaciÃ³n
â”‚   â”‚   â”œâ”€â”€ controllers/
â”‚   â”‚   â”‚   â”œâ”€â”€ models.controller.js   # LÃ³gica de modelos
â”‚   â”‚   â”‚   â””â”€â”€ generate.controller.js # LÃ³gica de generaciÃ³n
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ ollama.service.js # Cliente HTTP para FastAPI
â”‚   â”‚   â”œâ”€â”€ middlewares/
â”‚   â”‚   â”‚   â””â”€â”€ error.middleware.js # Manejo de errores
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚       â””â”€â”€ logger.js        # Sistema de logging
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ .env
â”‚   â””â”€â”€ README.md
â”‚
â””â”€â”€ fastapi/                      # Backend de IA (FastAPI + Python)
    â”œâ”€â”€ app/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ main.py
    â”‚   â”œâ”€â”€ routes/
    â”‚   â”‚   â”œâ”€â”€ generate.py      # Endpoint de generaciÃ³n de cÃ³digo
    â”‚   â”‚   â””â”€â”€ models.py        # Endpoint de modelos
    â”‚   â”œâ”€â”€ services/
    â”‚   â”‚   â””â”€â”€ ollama_service.py # ComunicaciÃ³n con Ollama
    â”‚   â”œâ”€â”€ schemas/
    â”‚   â”‚   â””â”€â”€ generate_request.py # Modelos Pydantic
    â”‚   â””â”€â”€ core/
    â”‚       â”œâ”€â”€ config.py        # Variables de entorno
    â”‚       â””â”€â”€ logger.py        # Logging
    â”œâ”€â”€ requirements.txt
    â”œâ”€â”€ .env
    â””â”€â”€ README.md
```

---

## ğŸ§  Arquitectura de dos capas

### ğŸŸ¢ Node.js API Gateway (Puerto 3000)
Punto de entrada para el frontend. Se encarga de:
- Recibir peticiones del frontend React
- ValidaciÃ³n inicial y manejo de archivos
- Proxy a FastAPI para procesamiento de IA
- Futuras features: autenticaciÃ³n, historial de chats, gestiÃ³n de usuarios

### ğŸ”µ FastAPI (Puerto 8001)
Backend especializado en IA. Se encarga de:
- ComunicaciÃ³n con **Ollama** (modelos locales de IA)
- Procesamiento de imÃ¡genes y conversiÃ³n a base64
- GeneraciÃ³n de cÃ³digo hÃ­brido
- GestiÃ³n de modelos y timeouts configurables

---

## ğŸ§  FastAPI

Se encarga de comunicarse con **Ollama** (modelos locales de IA), procesar imaÌgenes y devolver respuestas estructuradas que posteriormente usaraÌ el **API Gateway en Node.js** y el **frontend en React**.

## âš™ï¸ QuÃ© hace cada mÃ³dulo

| Archivo | DescripciÃ³n |
|----------|--------------|
| `main.py` | Punto de entrada del servidor FastAPI. Registra las rutas, configura CORS y eventos de startup/shutdown. |
| `routes/generate.py` | Endpoint `/generate/` que recibe `model`, `prompt` y una `imagen` opcional. Valida entrada (max 10MB), llama a Ollama y devuelve el cÃ³digo generado. |
| `routes/models.py` | Endpoint `/models/` que devuelve la lista de modelos disponibles en Ollama con manejo de errores robusto. |
| `services/ollama_service.py` | LÃ³gica de comunicaciÃ³n con Ollama (`/api/chat`, `/api/tags`). Convierte imÃ¡genes a base64 y maneja mÃºltiples formatos de respuesta. |
| `schemas/generate_request.py` | Modelos Pydantic para validaciÃ³n de datos: `GenerateRequest`, `GenerateResponse`, `ModelInfo`, `ModelsResponse`. |
| `core/config.py` | Carga variables de entorno, configura URLs de Ollama y CORS con soporte para mÃºltiples orÃ­genes. |
| `core/logger.py` | Sistema de logging configurable con niveles y formato mejorado. |
| `requirements.txt` | Dependencias de Python necesarias. |

---

## ğŸ“‹ Requisitos Previos

1. **Python 3.10+**
2. **Pip** actualizado: `python -m pip install --upgrade pip`
3. **Ollama** instalado y ejecutÃ¡ndose localmente
   - ğŸ‘‰ [Descargar Ollama](https://ollama.ai)
   - Verificar instalaciÃ³n: `ollama --version`
4. **Al menos un modelo descargado**

## ğŸ” ConfiguraciÃ³n de Variables de Entorno

### Node.js API Gateway (.env en node/)

Crea un archivo `.env` en la carpeta `node/`:

```env
# FastAPI Backend
FASTAPI_URL=http://localhost:8001

# Server Configuration
PORT=3000
NODE_ENV=development

# Request Configuration
REQUEST_TIMEOUT=600000  # 10 minutos en ms (para modelos grandes)
MAX_FILE_SIZE=10485760  # 10MB en bytes

# CORS Configuration (comma-separated)
ALLOWED_ORIGINS=http://localhost:5173,http://localhost:5174

# Logging
LOG_LEVEL=info
```

### FastAPI Backend (.env en fastapi/)

Crea un archivo `.env` en la carpeta `fastapi/`:

```env
# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434

# Timeout Configuration (in seconds)
# For large models (27B+), increase OLLAMA_TIMEOUT to 600-900 seconds
OLLAMA_TIMEOUT=600
OLLAMA_TAGS_TIMEOUT=30

# Server Configuration
HOST=0.0.0.0
PORT=8001

# CORS Configuration (comma-separated list)
ALLOWED_ORIGINS=http://localhost:3000,http://localhost:5173

# Logging
LOG_LEVEL=INFO
```

### âš™ï¸ ConfiguraciÃ³n de Timeouts

- **REQUEST_TIMEOUT** (Node.js): Tiempo mÃ¡ximo en milisegundos para requests HTTP
- **OLLAMA_TIMEOUT** (FastAPI): Tiempo mÃ¡ximo de espera para generaciÃ³n de cÃ³digo
  - Modelos pequeÃ±os (7B): 60-120 segundos
  - Modelos medianos (13B): 120-300 segundos
  - Modelos grandes (27B+): 600-900 segundos
  
- **OLLAMA_TAGS_TIMEOUT** (FastAPI): Tiempo de espera para listar modelos (default: 30s)

**Nota:** Los scripts de setup crean automÃ¡ticamente estos archivos si no existen.

---

## ğŸš€ InstalaciÃ³n y EjecuciÃ³n Local

### ğŸ“¦ InstalaciÃ³n completa (ambos servicios)

```bash
# 1. Instalar y configurar FastAPI
cd tfg-backend/fastapi
./setup.sh  # Crea .venv, instala dependencias, crea .env
cd ..

# 2. Instalar y configurar Node.js
cd node
npm install  # Instala todas las dependencias
cd ..
```

### â–¶ï¸ EjecuciÃ³n completa

```bash
# Terminal 1: Iniciar Ollama (debe estar corriendo siempre)
ollama serve

# Terminal 2: Iniciar FastAPI
cd tfg-backend/fastapi
./run.sh

# Terminal 3: Iniciar Node.js API Gateway
cd tfg-backend/node
npm run dev
```

### ğŸ”µ FastAPI - InstalaciÃ³n Manual

#### 1ï¸âƒ£ Navegar a la carpeta

```bash
cd tfg-backend/fastapi
```

#### 2ï¸âƒ£ Crear entorno virtual

```bash
python -m venv .venv
```

#### 3ï¸âƒ£ Activar el entorno

**macOS / Linux:**
```bash
source .venv/bin/activate
```

**Windows (PowerShell):**
```bash
.venv\Scripts\Activate.ps1
```

#### 4ï¸âƒ£ Instalar dependencias

```bash
pip install -r requirements.txt
```

#### 5ï¸âƒ£ Crear archivo `.env`

Copia el ejemplo de la secciÃ³n anterior.

### EjecuciÃ³n de FastAPI

**OpciÃ³n A: Script automÃ¡tico**
```bash
./run.sh
```

**OpciÃ³n B: Manual**
```bash
source .venv/bin/activate
uvicorn app.main:app --host 0.0.0.0 --port 8001 --reload
```

---

## ğŸ§ª Endpoints disponibles

### ğŸŒ Node.js API Gateway (Puerto 3000)

Todos los endpoints del frontend deben apuntar a `http://localhost:3000/api`

#### ğŸŸ¢ GET /api/models

**DescripciÃ³n:** Obtiene la lista de modelos disponibles en Ollama

**Request:**
```bash
curl http://localhost:3000/api/models
```

**Response:**
```json
{
  "models": [
    {
      "name": "gemma3:27b",
      "modified_at": "2024-01-15T10:30:00Z",
      "size": 27000000000
    }
  ]
}
```

#### ğŸŸ¢ POST /api/generate

**DescripciÃ³n:** Genera cÃ³digo a partir de texto o imagen

**Request con texto:**
```bash
curl -X POST http://localhost:3000/api/generate \
  -F "model=qwen2.5-coder:14b" \
  -F "prompt=Crea un hola mundo en python"
```

**Request con imagen:**
```bash
curl -X POST http://localhost:3000/api/generate \
  -F "model=qwen3-vl:8b" \
  -F "prompt=Generate the PlantUML code from this diagram" \
  -F "image=@./iterator.png"
```

**Response:**
```json
{
  "model": "gemma3:27b",
  "content": "public class User { ... }"
}
```

#### ğŸŸ¢ POST /api/models/unload

**DescripciÃ³n:** Descarga un modelo de la memoria

**Request:**
```bash
curl -X POST http://localhost:3000/api/models/unload \
  -H "Content-Type: application/json" \
  -d '{"model": "qwen3-vl:8b"}'
```

### ğŸ”§ FastAPI (Puerto 8001)

Endpoints directos de FastAPI (normalmente solo usados internamente por el API Gateway)

#### ğŸŸ¢ GET / - Health Check


**DescripciÃ³n:** Verifica que el servidor estÃ¡ activo.

**Ejemplo:**
```bash
curl http://localhost:8001/
```

**Respuesta:**
```json
{
  "message": "FastAPI IA service running",
  "status": "ok"
}
```

---

### ğŸŸ£ GET /models/ - Listar Modelos

**DescripciÃ³n:** Devuelve la lista de modelos disponibles en Ollama.

**Ejemplo:**
```bash
curl http://localhost:8001/models/
```

**Respuesta esperada:**
```json
{
  "models": [
    {
      "name": "qwen2-vl:latest",
      "modified_at": "2024-01-15T10:30:00Z",
      "size": 4400000000,
      "digest": "sha256:abc123..."
    }
  ]
}
```

**Errores posibles:**
- `503` - Ollama no estÃ¡ corriendo o no responde

---

### ğŸ—‘ï¸ POST /models/unload - Descargar Modelo de Memoria

**DescripciÃ³n:** Descarga un modelo de la memoria RAM/VRAM para liberar recursos del sistema.

**Request Body:**
```json
{
  "model": "qwen2-vl"
}
```

**Ejemplo:**
```bash
curl -X POST "http://localhost:8001/models/unload" \
  -H "Content-Type: application/json" \
  -d '{"model": "qwen3-vl:8b"}'
```

**Respuesta esperada:**
```json
{
  "success": true,
  "message": "Modelo qwen2-vl descargado de memoria exitosamente",
  "model": "qwen2-vl"
}
```

**Errores posibles:**
- `503` - Ollama no estÃ¡ corriendo o el modelo no existe
- `500` - Error al descargar el modelo

---

### ğŸ”µ POST /generate/ - Generar CÃ³digo

**DescripciÃ³n:** Genera cÃ³digo a partir de un prompt y opcionalmente una imagen (diagrama UML).

**ParÃ¡metros:**
- `model` (string, requerido): Nombre del modelo en Ollama
- `prompt` (string, requerido): DescripciÃ³n de lo que quieres generar
- `image` (file, opcional): Imagen del diagrama UML (mÃ¡x 10MB)

**Ejemplo sin imagen:**
```bash
curl -X POST "http://localhost:8001/generate/" \
  -F "model=qwen3-vl:8b" \
  -F "prompt=Crea una hola mundo en python"
```

**Ejemplo con imagen:**
```bash
curl -X POST "http://localhost:8001/generate/" \
  -F "model=qwen3-vl:8b" \
  -F "prompt=Analiza el diagrama UML y devuelve Ãºnicamente el cÃ³digo PlantUML correspondiente" \
  -F "image=@/ruta/a/diagrama.png"
```

**Respuesta esperada:**
```json
{
  "result": "class UserManager:\n    def __init__(self):\n        self.users = []\n    \n    def create_user(self, name, email):\n        ..."
}
```

**Errores posibles:**
- `400` - Imagen demasiado grande (>10MB) o parÃ¡metros invÃ¡lidos
- `500` - Error en la generaciÃ³n o modelo no disponible
- `503` - Ollama no estÃ¡ corriendo

---

### ğŸ“š DocumentaciÃ³n Interactiva

FastAPI genera documentaciÃ³n automÃ¡tica:

- **Swagger UI**: http://localhost:8001/docs
- **ReDoc**: http://localhost:8001/redoc

Desde Swagger UI puedes probar todos los endpoints directamente.

---

## ğŸ§° Estructura de respuesta de Ollama

Este servicio interpreta automÃ¡ticamente las respuestas mÃ¡s comunes del API de Ollama:

**Formato 1:**
```json
{
  "message": {
    "content": "cÃ³digo generado..."
  }
}
```

**Formato 2:**
```json
{
  "choices": [
    {
      "message": {
        "content": "cÃ³digo generado..."
      }
    }
  ]
}
```

**Formato 3:**
```json
{
  "response": "cÃ³digo generado..."
}
```

El servicio maneja automÃ¡ticamente estos formatos y extrae el contenido en la funciÃ³n `_extract_content()` de `app/routes/generate.py`.

---

## ğŸ”„ Flujo de EjecuciÃ³n

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Usuario   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ POST /generate/
       â”‚ (model, prompt, image?)
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI Server  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ ValidaciÃ³n â”‚  â”‚ â† TamaÃ±o imagen, parÃ¡metros
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â†“        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Ollama    â”‚  â”‚ â† Convierte imagen a base64
â”‚  â”‚  Service   â”‚  â”‚   Construye payload
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚ HTTP POST
          â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Ollama  â”‚ â† Modelo de IA local
    â”‚  Server  â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
           â”‚ Respuesta JSON
           â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Extract    â”‚ â† Maneja 3 formatos
    â”‚  Content    â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Response   â”‚ â†’ {"result": "cÃ³digo..."}
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
---

## ï¿½ SoluciÃ³n de Problemas Comunes

### Timeout con modelos grandes

**Problema:** Error de timeout al usar modelos grandes (gemma3:27b, llama3:70b, etc.)

**SÃ­ntomas:**
```
requests.exceptions.ReadTimeout: HTTPConnectionPool(host='localhost', port=11434): 
Read timed out. (read timeout=600)
```

**SoluciÃ³n:**

1. **Aumentar el timeout en `.env`:**
```env
# Para modelos 27B+
OLLAMA_TIMEOUT=900

# Para modelos 70B+
OLLAMA_TIMEOUT=1800
```

2. **Reiniciar el servidor:**
```bash
# Detener el servidor (Ctrl+C)
# Reiniciar
./run.sh
```

3. **Verificar que se aplicÃ³:**
```bash
# Los logs deberÃ­an mostrar:
# "Calling Ollama with model: gemma3:27b (timeout: 900s)"
```

**Recomendaciones por tamaÃ±o de modelo:**
- **7B** (llama3.2, mistral): 60-120 segundos
- **13B** (llama3.1:13b): 120-300 segundos
- **27B** (gemma3:27b): 600-900 segundos
- **70B+** (llama3:70b): 1200-1800 segundos (20-30 min)

---

### Request timeout en el cliente

**Problema:** El cliente (curl, navegador) se desconecta antes de recibir respuesta.

**SoluciÃ³n con curl:**
```bash
# Aumentar el timeout del cliente
curl --max-time 900 -X POST "http://localhost:8001/generate/" \
  -F "model=gemma3:27b" \
  -F "prompt=Tu prompt" \
  -F "image=@imagen.png"
```

**SoluciÃ³n en cÃ³digo JavaScript/TypeScript:**
```javascript
// Aumentar timeout en fetch
const controller = new AbortController();
const timeoutId = setTimeout(() => controller.abort(), 900000); // 15 min

fetch('http://localhost:8001/generate/', {
  method: 'POST',
  body: formData,
  signal: controller.signal
})
  .finally(() => clearTimeout(timeoutId));
```

---

### Imagen demasiado grande

**Error:** `400 - Imagen demasiado grande. MÃ¡ximo 10MB`

**SoluciÃ³n:**
1. Reducir el tamaÃ±o de la imagen
2. O modificar el lÃ­mite en `app/routes/generate.py`:
```python
# LÃ­nea ~35
if len(image_bytes) > 20 * 1024 * 1024:  # Aumentar a 20MB
```

---

### Modelo no cargado/Respuesta lenta

**Problema:** La primera peticiÃ³n a un modelo tarda mucho.

**Causa:** Ollama carga el modelo en memoria en la primera peticiÃ³n.

**SoluciÃ³n:**
```bash
# Pre-cargar el modelo antes de usarlo
ollama run gemma3:27b "test"
# Ctrl+D para salir

# Ahora el modelo estÃ¡ en memoria y responderÃ¡ mÃ¡s rÃ¡pido
```

---

## ï¿½ğŸ“š Recursos

- [DocumentaciÃ³n de FastAPI](https://fastapi.tiangolo.com/)
- [DocumentaciÃ³n de Ollama](https://ollama.ai/docs)
- [Pydantic Documentation](https://docs.pydantic.dev/)
- [Modelos disponibles en Ollama](https://ollama.ai/library)
- [GuÃ­a de APIs REST](https://restfulapi.net/)

---

## ğŸ“Š TecnologÃ­as Utilizadas

- **FastAPI** 0.100+ - Framework web moderno y rÃ¡pido
- **Pydantic** V2 - ValidaciÃ³n de datos
- **Uvicorn** - Servidor ASGI
- **Python-multipart** - Manejo de archivos multipart
- **Requests** - Cliente HTTP
- **Python-dotenv** - GestiÃ³n de variables de entorno
- **Pillow** - Procesamiento de imÃ¡genes
- **Ollama** - Servidor de modelos de IA local

---

## ğŸ“„ Licencia

Hacer mÃ¡s adelante

---

## ğŸ“§ Contacto

- ğŸ“§ fernandomm1840@gmail.com